{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um algoritmo para problemas de Classificação. Recebe esse nome por ser baseado no teorema da probailidade de Bayes. O algoritimo tem como objetivo calcular a probabilidade de que uma amostra desconhecida pertença à uma das classes possíveis, ou seja, predizer a classe mais provável. Esse tipo de regressão é chamado de classificação estatística pois é baseada totalmente em probabilidade. Considera que o valor de um atributo sobre determinada classe é independente dos damais atributos, o que simplifica os cálculos envolvidos.\n",
    "\n",
    "O modelo Naive Bayes é um algoritmo de classificação extremamente rápido e simples, adequado para conjuntos de dados de grandes dimensões. O preço pago por essa eficiência é que o modelo geralmente fornece um desempenho de generalização um pouco pior que os classificadores lineares, como LogisticRegression e LinearSVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possíveis aplicações**  \n",
    "* Previsões multi-classes\n",
    "* Classificação de textos/Filtragem de spam/Análise de sentimento\n",
    "* Previsões em tempo real\n",
    "* Sistema de Recomendação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema de Bayes**  \n",
    "Em teoria da probabilidade o teorema de bayes mostra a relação entre a probabilidade condicional e a sua inversa.\n",
    "\n",
    "A essência do teorema de Bayes consiste na revisão das probabilidades iniciais (a priori) em relação à evidência amostral. As estimativas revisadas chamam-se probabilidades a posteriori. \n",
    "\n",
    "A equação acima se lê da seguinte forma: Probabilidade de um evento A, dado B é igual à Probabilidade de um evento B, dado A, multiplicado pela probabilidade de A, tudo isso sendo dividido pela probabilidade de B.\n",
    "O teorema, na prática, busca calcular as probabilidades condicionais do evento A, no numerador, dividido pelo total da probabilidade do evento B no denominador.\n",
    "\n",
    "Pr(A) e Pr(B) são as probabilidades a priori de A e B.\n",
    "Pr(B|A) e Pr(A|B) são as probabilidades a posteriori de B condicional a A e de A condicional a B\n",
    "respectivamente.\n",
    "\n",
    "<img src=\"figuras/naive-bayes-eq1.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por que Naive?**  \n",
    "A tradução aproximada de naive é ingênuo/simples, e usamos esse termo devido à algumas suposições que fazemos para calcular as probabilidades. O termo naive vem do fato de que se assume que cada recurso é independente do resto, ou seja, o valor de um recurso não tem relação com o valor de outro recurso.  \n",
    "\n",
    "Para facilitar os cálculos, atribuimos características à distribuição dos dados que podem assumir:  \n",
    "\n",
    "**No Scikit-Learn**\n",
    "* Gaussian - Assume a distribuição normal. ``from sklearn.naive_bayes import GaussianNB``  \n",
    "* Multinomial - Usado para contagem de variáveis discretas. ``from sklearn.naive_bayes import MultinomialNB``  \n",
    "* Bernoulli - Ou binomial, onde possuímos como saída uma classificação binária. ``from sklearn.naive_bayes import BernoulliNB``\n",
    "\n",
    "O ``MultinomialNB`` leva em consideração o valor médio de cada recurso para cada classe, enquanto o ``GaussianNB`` armazena o valor médio, bem como o desvio padrão de cada recurso para cada classe. O classificador ``BernoulliNB`` conta quantas vezes cada recurso de cada classe não é zero.\n",
    "\n",
    "O treinamento do modelo é reduzido ao cálculo das probabilidades condicionais envolvidas, que podem ser estimadas pela contagem de frequências de correlações entre valores de recursos e valores de classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorando o poder do Naive Bayes**\n",
    "* Se os atributos contínuos não têm distribuição normal, devemos usar a transformação ou métodos diferentes para convertê-los em distribuição normal.\n",
    "* Se o conjunto de dados de teste tem problema de frequência zero, aplique a técnica de suavização “Laplace Correction” para  rever a classe no conjunto de dados de teste.\n",
    "* Remova variáveis correlacionadas. Os atributos altamente correlacionadas podem levar a um excesso de importância de uma característica, reduzindo a capacidade de generalização do modelo.\n",
    "* Classificadores Naive Bayes têm opções limitadas para ajuste de parâmetros, tais como como **alfa = 1 para suavização**, fit_prior = [Verdadeiro | Falso] para aprendizagem a partir de probabilidades anteriores. Foque no pré-processamento de dados e seleção de atributos.\n",
    "* Você pode querer aplicar alguma técnica ensemble como “bagging” e “boosting”, mas na prática esses métodos não ajudariam, pois a finalidade destes métodos é reduzir a variância. Naive Bayes não tem variância para minimizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vantagens**\n",
    "* É fácil e rápido para prever o conjunto de dados da classe de teste. Também tem um bom desempenho na previsão de classes múltiplas.\n",
    "* Quando a suposição de independência prevalece, um classificador Naive Bayes tem melhor desempenho em comparação com outros modelos como regressão logística, e você precisa de menos dados de treinamento.\n",
    "* O desempenho é bom em caso de variáveis categóricas de entrada em comparação a variáveis numéricas. Para variáveis numéricas, assume-se a distribuição normal (curva de sino, que é uma suposição forte).\n",
    "\n",
    "**Desvantagens**\n",
    "* Se a variável categórica tem uma categoria (no conjunto de dados de teste) que não foi observada no conjunto de dados de treinamento, então o modelo irá atribuir uma probabilidade de 0 (zero) e não será capaz de fazer uma previsão. Isso é muitas vezes conhecido como “Zero Frequency”. Para resolver esse problema, podemos usar a técnica de ”suavização” (smoothing). Uma das técnicas mais simples de ”suavização” (smoothing) é a chamada estimativa de Laplace.\n",
    "* Uma limitação do Naive Bayes é a suposição de preditores independentes. Na vida real, é quase impossível ter um conjunto de indicadores que sejam completamente independentes.  \n",
    "\n",
    "O modelo GaussianNB é raramente usado por praticantes, enquanto que as outras duas variantes são amplamente usadas para dados de contagem esparsa, como texto. Geralmente, o MultinomialNB tem um desempenho melhor do que o BinaryNB, em particular em conjuntos de dados com um número relativamente grande de recursos diferentes de zero (por exemplo, documentos grandes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para ir além**  \n",
    "Naive Bayes and Text Classification I - Introduction and Theory  \n",
    "https://arxiv.org/pdf/1410.5329v3.pdf  \n",
    "\n",
    "Baye’s Theorem  \n",
    "http://faculty.washington.edu/tamre/BayesTheorem.pdf  \n",
    "\n",
    "Total Probability and Bayes’ Theorem  \n",
    "http://www.personal.soton.ac.uk/jav/soton/HELM/workbooks/workbook_35/35_4_tot_prob_bayes_thm.pdf  \n",
    "\n",
    "Naïve Bayes  \n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html  \n",
    "\n",
    "The Bayesian Algorithm  \n",
    "https://www.blackwellpublishing.com/content/BPL_Images/Content_store/Sample_chapter/9781405117197/Lancaster_sample%20chapter_Intro%20to%20modern%20Bayesian%20Econometrics.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
