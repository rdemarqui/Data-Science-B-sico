{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM’s) é modelo de aprendizagem supervisionada, capaz de realizar classificação linear, não-linear, regressão e até mesmo detecção de outliers.\n",
    " \n",
    "As SVMs são comparadas às redes neurais, podendo assim ter um modelo preditivo similar em muitos problemas. Entram na categoria de algoritmos de Machine Learning chamadas black box, pois seu funcionamento não é tão simples de ser compreendido e muitas vezes é dificil entender como o algoritmo gerou o resultado esperado. São particularmente adequadas para classificação de conjuntos de dados complexos, mas pequenos ou médios.\n",
    "\n",
    "<img src=\"figuras/svm01.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metodologia**  \n",
    "Em primeiro lugar o algoritmo classifica os dados em 2 grupos (considerando uma classificação binária). Em seguida, o algoritmo gera uma linha que divide as duas classes, chamada hiperplano. Entretanto pode haver mais de uma linha que divide corretamente os dados, dessa forma o algoritmo escolhe o hiperplano que **maximiza as margens entre as classes**, ou seja, escolhe o hiperplano que apresentar a maior distância entre as duas categorias classificadas, dessa forma ele consegue generalizar o modelo.\n",
    "<img src=\"figuras/svm02.png\" width=\"300\"/>\n",
    "\n",
    "Os pontos que estão sobre as margens são conhecidos como vetores de suporte, vindo daí o nome do algoritmo (support vector). Os vetores de suporte são as coordenadas de uma observação individual, que estão sobre as margens da fronteira que segrega melhor as duas classes. Os outros elementos longe da fronteira não são relevantes para a solução. Isso reduz a variação do modelo porque a solução é insensível à remoção de elementos que não sejam os elementos de suporte.\n",
    "<img src=\"figuras/svm03.png\" width=\"430\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante** - O SVM é sensivel à escala dos dados, portanto em casos onde os dados estejam em escalas diferentes é importante aplicar feature scaling, com ``StandardScaler``, para obtermos um resultado melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs com Margens Rígidas x Margens Flaxíveis\n",
    "Um dos métodos mais utilizados bara busca de parâmetros de forma automatica, quando o conhecimento prévio do conjunto dos dados não existe, é a busca em grade ou grid search. As margens possuem um papael fundamenteal nesse processo e um de seus preceitos é garantir que não haverá dados no interior das margens calculadas. Existem basicamente 2 tipos de margens:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM’s com Margens Rígidas (hard margin classification)**  \n",
    "Procura separar os dados perfeitamente através de funções lineares. Existem dois problemas principais com classificação de margem rígida. Primeiro, ele só funciona se os dados forem linearmente separáveis e, em segundo lugar, é bastante sensível a valores discrepantes.\n",
    "<img src=\"figuras/svm14.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM’s com Margens Flexíveis (soft margin classification)**  \n",
    "Para evitar esses problemas, é preferível usar um modelo mais flexível. O objetivo é encontrar um bom equilíbrio entre manter as margens mais largas possível e limitar as violações dessas margens (ou seja, situações que acabam entre as margens ou até no lado errado).\n",
    "No Scikit-Learn, você pode controlar esse equilíbrio usando o hyperparametro C, explicado com mais detalhes a seguir.\n",
    "\n",
    "<img src=\"figuras/svm16.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Lineares x Não Lineares\n",
    "O SVM é bastante versátil, pois pode se ajustar a modelos lineares e não lineares, graças às funções especiais chamadas **kernel**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVMs Lineares** - Podemos utilizar quando tivermos apenas duas classes de dados.\n",
    "<img src=\"figuras/svm08.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Não-Lineares**  \n",
    "As **funções kernel** são capazes de mapear os atributos de entrada em um novo vetor de atributos mais complexo, utilizando uma quantidade mais limitada de cálculos.\n",
    "\n",
    "<img src=\"figuras/svm09.png\" width=\"550\"/>\n",
    "\n",
    "A idéia básica por trás dos métodos kernel para lidar com esses dados linearmente inseparáveis é criar combinações não-lineares dos recursos originais para projetá-los em um espaço dimensional superior, onde ele se torna linearmente separável.\n",
    "\n",
    "<img src=\"figuras/svm10.png\" width=\"550\"/>\n",
    " \n",
    "As funções Kernel mais utilizadas são as **polinomiais** e **base radial**. A priori não há uma maneira de saber qual função obterá a melhor precisão. Como qualquer modelo de aprendizagem supervisionada, primeiro treinamos uma máquina de vetores de suporte e, em seguida, validamos o classificador. Para conseguir um nível de acurácia satisfatório, precisamos fazer o tuning dos parâmetros das funções de kernel.\n",
    "\n",
    "<img src=\"figuras/svm11.png\" width=\"300\"/>\n",
    "\n",
    "**Boa prática** - Quando trabalhamos com um dataset com quantidade de atributos > 1000, sugere-se que se trabalhe com kernel = linear, pois é provável que os dados sejam linearmente separáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Trick**  \n",
    "Utilizar uma função kernel para transformar um conjunto não-linear em linear é conhecido como **Kernel trick** (truque kernel).\n",
    "\n",
    "A figura abaixo mostra o dataset Iris com classes que não são possíveis de separar linearmente: \n",
    "<img src=\"figuras/svm12.png\" width=\"750\"/>\n",
    "\n",
    "Nesse caso, para facilitar a separabilidade linear, a função de kernel aumenta a dimensão de espaço, permitindo utilizar uma linha reta que separe os conjuntos de dados. No exemplo abaixo, para criar um terceiro eixo (Z), a feature 2 foi elevada ao quadrado:\n",
    "\n",
    "X: Feature 1; Y: Feature 2; Z: Feature 2 ** 2 <img src=\"figuras/svm13.png\" width=\"800\"/>\n",
    "\n",
    "Embora a dimensão do espaço aumente em Z, a complexidade diminui, porque a classificação, que no espaço de entrada (X) só era possível utilizando superfícies de decisão não-lineares, no espaço de características (Z), pode ser feita apenas com um simples hiperplano (superfície de decisão linear).\n",
    "\n",
    "No exemplo acima, transformamos dados bidimensionais em tridimensionais, porém o kernel trick pode tranformar os dados em 4 ou mais dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros\n",
    "**Parâmetro C**  \n",
    "Usando a variável C, podemos controlar a penalidade por erro de classificação. Explicando de uma maneira direta, o parâmetro C define a largura das margens em relção ao limite de decisão.  \n",
    "\n",
    "Um valor de C alto (exemplo abaixo à esq.), a margem se tornará mais estreita, e as amostras violarão pouco os limites (hard).  \n",
    "Um valor de C baixo (exemplo abaixo à dir), a margem será mais larga, mas muitas amostras acabarão violando os limites (soft).\n",
    "\n",
    "Um classificador com margens mais largas (C baixo) provavelmente irá generalizar melhor. Observando o exemplo abaixo, o caso à direita fez menos erros de previsão, uma vez que a maioria das violações de margem estão, na verdade, no lado correto do limite de decisão.\n",
    "\n",
    "<img src=\"figuras/svm17.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parâmetro Gama**  \n",
    "O parâmetro gama ajuda o kernel a verificar o nível de influências dos vetores de suporte. É um coeficiente de kernel para 'rbf', 'poli' e 'sigmóide'.\n",
    "\n",
    "Valor baixo significa 'alta variância' e maior influência do vetor de suporte.  \n",
    "Valor alto significa 'baixa variância' e os vetores de suporte não possuem grande influência no processo de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, uma amostra de várias configurações dos parâmetros gama e C no Iris dataset:\n",
    "\n",
    "<img src=\"figuras/svm15.png\" width=\"700\"/>\n",
    "\n",
    "Indo da esquerda para a direita, aumentamos o parâmetro gama de 0,1 para 10. Um gama alto implica num raio grande para o kernel Gaussiano, o que significa que muitos pontos serão considerados próximos. Isso se reflete em limites de decisão muito suaves à esquerda e limites que se concentram mais em pontos únicos mais à direita. Um valor baixo de gama significa que o limite de decisão irá variar lentamente, o que produz um modelo de baixa complexidade, enquanto um valor alto de gama produz um modelo mais complexo.\n",
    "\n",
    "Indo de cima para baixo, aumentamos o parâmetro C de 0,1 para 1000. Assim como nos modelos lineares, um C pequeno significa um modelo muito restrito, em que cada ponto de dados só pode ter uma influência muito limitada. Você pode ver que, no canto superior esquerdo, o limite de decisão parece quase linear, com os pontos vermelho e azul que são classificados incorretamente, quase não alterando a linha.\n",
    "\n",
    "Aumentar C, como mostrado na parte inferior direita, permite que esses pontos tenham uma influência mais forte no modelo e faz com que a decisão limite se dobre para classificá-los corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens e Desvantagens do SVM\n",
    "\n",
    "**Vantagens**  \n",
    "- Eficaz em espaços dimensionais elevados.  \n",
    "- Usa um subconjunto de pontos de treinamento na função de decisão (chamados vetores de suporte), o que o torna eficiente na utilização de memória.  \n",
    "- Uma vez que o modelo é treinado, a fase de predição é muito rápida.\n",
    "- Versátil: diferentes funções de Kernel podem ser especificadas para a função de decisão. Os kernels mais comuns são fornecidos, mas também é possível especificar kernels personalizados.\n",
    "\n",
    "**Desvantagens**\n",
    "- Não desempenha muito bem em datasets com alto número de amostras. Executar SVM em dados com até 10.000 amostras pode funcionar bem, mas trabalhar com conjuntos de dados de tamanho 100.000 ou mais pode se tornar um desafio em termos de tempo de execução e uso de memória. \n",
    "- Os resultados são fortemente dependentes da escolha adequada do parâmetro de suavização C. Isso pode ser feito através da validação cruzada, porém o processamento pode ficar demorado à medida que os conjuntos de dados aumentam de tamanho.  \n",
    "- As SVM’s não fornecem diretamente estimativas de probabilidade, estas são calculadas usando uma validação cruzada (cross-validation).\n",
    "- Sensíveis à escala, sendo necessário normalizar os dados antes de aplicar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações finais\n",
    "Em SVM o objetivo de otimização é maximizar a margem. A complexidade da construção do modelo depende do número de vetores de suporte e não da dimensão do espaço de características.\n",
    "Antes de aplicar uma SVM para classificar um conjunto de dados, é necessário responder algumas questões:\n",
    "- Qual função de kernel utilizar?\n",
    "- Qual o valor do parâmetro C (Soft Margin)?\n",
    "- Qual o valor gamma?\n",
    "\n",
    "Uma abordagem comum para encontrar os valores corretos dos hiperparâmetros é usar a grid search. Ter um bom senso do que cada hiperparâmetro realmente faz também pode ajudá-lo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
