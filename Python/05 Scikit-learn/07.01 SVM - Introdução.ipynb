{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM’s) são modelos de aprendizagem supervisionada, que possuem algoritmos de aprendizagem que analisam dados e reconhecem padrões, utilizados para classificação e análise de regressão. É bastante versátil, pois pode se ajustar a modelos lineares e não lineares, graças as funções especiais chamadas kernel.  \n",
    "As funções kernel são capazes de mapear os atributos de entrada em um novo vetor de atributos mais complexo, utilizando uma quantidade mais limitada de cálculos.  \n",
    "As SVMs são comparadas às redes neurais, podendo assim ter um modelo preditivo similar em muitos problemas. Entram na categoria de algoritmos de Machine Learning chamadas black box, pois seu funcionamento não é tão simples de ser compreendido e muitas vezes é dificil entender como o algoritmo gerou o resultado esperado.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O SVM padrão toma como entrada um conjunto de dados e prediz para cada nova entrada qual de duas possíveis classes aquela entrada faz parte, o que torna o SVM um **classificador linear binário não probabilístico.**\n",
    "\n",
    "<img src=\"figuras/svm01.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algumas características das SVM’s:**\n",
    "- Em caso de outliers a SVM busca a melhor forma possível de classificação e, se necessário, desconsidera o outlier;\n",
    "- É um classificador criado para fornecer separação linear;\n",
    "- Funciona muito bem em domínios complicados, em que existe uma clara margem de separação;\n",
    "- Não funciona bem em conjuntos de dados muito grandes, pois o tempo de treinamento é muito custoso;\n",
    "- Não funciona bem em conjuntos de dados com grande quantidade de ruídos;\n",
    "- Se as classes estiverem muito sobrepostas deve-se utilizar apenas evidências independentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metodologia**  \n",
    "Em primeiro lugar o algoritmo classifica os dados em 2 grupos (considerando uma classificação binária). Em seguida, o algoritmo gera uma linha que divide as duas classes, chamada hiperplano. Entretanto pode haver mais de uma linha que divide corretamente os dados, dessa forma o algoritmo escolhe o hiperplano que maximiza as margens entre as classes, ou seja, escolhe o hiperplano que apresentar a maior distância entre as duas categorias classificadas, dessa forma ele consegue generalizar o modelo.\n",
    "<img src=\"figuras/svm02.png\" width=\"300\"/>\n",
    "\n",
    "Os pontos que estão sobre as margens são conhecidos como vetores de suporte, vindo daí o nome do algoritmo. Os vetores de suporte são as coordenadas de uma observação individual, que estão sobre as margens da fronteira que segrega melhor as duas classes. \n",
    "<img src=\"figuras/svm03.png\" width=\"430\"/>\n",
    "\n",
    "No caso acima, os dados são linearmente separáveis, porém as SVMs também permitem trabalhar com dados não-linearmente separáveis, através do kernel trick. O kernel trick nos permite adicionar mais de uma dimensão, permitindo utilizar uma linha reta que separa os conjuntos de dados.\n",
    "<img src=\"figuras/svm04.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante**  \n",
    "O SVM é sensivel à escala dos dados, portanto em casos onde os dados estejam em escalas diferentes é importante aplicar feature scaling, com ``StandardScaler``, para obtermos um melhor resultado.\n",
    "\n",
    "**Dica**  \n",
    "Quando trabalhamos com um dataset com quantidade de atributos > 1000, sugere-se que se trabalhe com kernel = linear, pois é provável que os dados sejam linearmente separáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teoria do aprendizado estatístico**  \n",
    "Considere f um classificador e F o conjunto de todos os classificadores que um determinado algoritmo de Machine Learning pode gerar. Esse algoritmo, durante o processo de aprendizado, utiliza um conjunto de treinamento T, composto de n pares (xi , yi), para gerar um classificador particular ˆf ∈ F.\n",
    "\n",
    "A teoria do aprendizado estatístico estabelece condições matemáticas que  auxiliam na escolha de um classificador particular a partir de um conjunto de dados de treinamento. Essas condições levam em conta o desempenho do classificador no conjunto de treinamento e a sua complexidade, com o objetivo de apresentar bons resultados a partir de novos dados.\n",
    "\n",
    "a) Overfitting b) Bom c) Underfitting\n",
    "<img src=\"figuras/svm05.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erro/Risco**  \n",
    "O erro, também conhecido como risco, esperado de um classificador f para dados de teste, podem ser quantificados pela equação abaixo:\n",
    "<img src=\"figuras/svm06.png\" width=\"300\"/>\n",
    "O risco esperado mede a capacidade de generalização, onde:  \n",
    "c(f(x) ,y) = equação de custo relacionada à previsão de f(x), onde a saída desejada é y. Essa função retorna o valor zero se x for classificado corretamente e 1 caso contrário.\n",
    "\n",
    "Infelizmente, não é possível minimizar o risco esperado apresentado na equação diretamente, uma vez que em geral a distribuição de probabilidade P(x; y) é desconhecida.\n",
    "\n",
    "O risco empírico de f, dado abaixo, mede o desempenho do classificador nos dados de treinamento, por meio da taxa de classificações incorretas obtidas a partir do conjunto de dados.\n",
    "<img src=\"figuras/svm07.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Lineares x Não Lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVMs Lineares** - Podemos utilizar quando tivermos apenas duas classes de dados.\n",
    "<img src=\"figuras/svm08.png\" width=\"550\"/>\n",
    "\n",
    "**SVM Não-Lineares**\n",
    "<img src=\"figuras/svm09.png\" width=\"550\"/>\n",
    "\n",
    "<img src=\"figuras/svm10.png\" width=\"550\"/>\n",
    "\n",
    "Essa transformação em um terceiro eixo é conhecida como **Funções Kernel**.  \n",
    "As funções Kernel mais utilizadas são as polinomiais e base radial. A priori não há uma maneira de saber qual função kernel obterá a melhor precisão.  \n",
    "Como qualquer modelo de aprendizagem supervisionado, você primeiro treina uma máquina de vetores de suporte e, em seguida, valida o classificador.  \n",
    "Para conseguir um nível de acurácia satisfatório, precisamos fazer o tuning dos parâmetros das funções de kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos Kernel\n",
    "Para muitos algoritmos, os dados em representação bruta têm de ser explicitamente transformados em representações de vetor de atributos por meio de um mapa de características especificado pelo cientista de dados (dados de treinamento). Em contraste o método kernel requer apenas um kernel especificado pelo cientista de dados, ou seja, uma função de similaridade sobre pares de pontos de dados.\n",
    "\n",
    "**Truque de Kernel**: Substitui os atributos (preditores) por uma função Kernel.\n",
    "<img src=\"figuras/svm11.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funções de Kernel e Kernel Trick**\n",
    "Em alguns problemas não é possível separar as classes linearmente, mesmo utilizando as margens de folga do hiperplano. Nesse caso, para facilitar a separabilidade linear, a função de kernel aumenta a dimensão de espaço. Esse procedimento é conhecido como Kernel trick (truque kernel).\n",
    "<img src=\"figuras/svm12.png\" width=\"300\"/>\n",
    "X = Espaço de Entrada; Z= Espaço de Característica\n",
    "\n",
    "\n",
    "Embora a dimensão do espaço aumente em Z, a complexidade diminui, porque a classificação, que no espaço de entrada (X) só era possível utilizando superfícies de decisão não lineares, no espaço de características (Z), pode ser feita apenas com um simples hiperplano (superfície de decisão linear).\n",
    "\n",
    "Funções Kernel:<img src=\"figuras/svm13.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Em SVM a complexidade da construção do modelo depende do número de vetores de suporte e não da dimensão do espaço de características.\n",
    "Antes de aplicar uma SVM para classificar um conjunto de dados é necessário responder algumas questões:\n",
    "- Quais funções de kernel utilizar?\n",
    "- Qual o valor do parâmetro C (Soft Margin)?\n",
    "- Qual o valor gamma?\n",
    "\n",
    "- Validações cruzadas (cross‐validations) devem ser utilizadas para evitar overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs com Margens Rígidas x Margens Flaxíveis\n",
    "Um dos métodos mais utilizados bara busca de parâmetros de forma automatica, quando o conhecimento prévio do conjunto dos dados não existe, é a busca em grade ou grid search. As margens possuem um papael fundamenteal nesse processo e um de seus preceitos é garantir que não haverá dados no interior das margens calculadas. Existem basicamente 2 tipos de margens:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM’s com Margens Rígidas (hard margin classification)**  \n",
    "Procura separar os dados perfeitamente através de funções lineares.\n",
    "<img src=\"figuras/svm14.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM’s com Margens Flexíveis (soft margin classification)**  \n",
    "Em situações reais é difícil encontrar aplicações cujos dados sejam linearmente separáveis. Nesse casos as SVMs de Margens Rígidas são extendidas para lidar com o conjunto de treinamento mais geral. Para realizar essa tarefa permite-se que alguns dados possam violar a restrição imposta pelas SVMs de Margens rígidas e isso é feito com a introdução de variáveis de folga na equação que calcula as margens. Essas variáveis relaxam as restrições impostas ao problema de otimização primal. A aplicação desse procedimento suaviza a as margens do classificador linear permitindo que alguns dados permanecam entre as margens do hiperplano.\n",
    "\n",
    "<img src=\"figuras/svm15.png\" width=\"550\"/>\n",
    "A constante C é um dos termos de regularização que impõe um peso à minimização dos erros no conjunto de treinamento. O que pode ser visto como a minimização de erros marginais. C é um dos parâmetros do algoritmo SVM tanto em Python quanto em R.\n",
    "\n",
    "<img src=\"figuras/svm16.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros\n",
    "**Parâmetro C**  \n",
    "Explicando de uma maneira direta, o parâmetro C define a largura das margens em relção ao limite de decisão.  \n",
    "\n",
    "Um valor de C alto (exemplo abaixo à esq.), a margem se tornará mais estreita, e as amostras violarão pouco os limites (hard).  \n",
    "Um valor de C baixo (exemplo abaixo à dir), a margem será mais larga, mas muitas amostras acabarão violando os limites (soft).\n",
    "\n",
    "No entanto, parece provável que um classificador com margens mais largas (C baixo) irá generalizar melhor. Observando o exemplo abaixo, o caso à direita fez menos erros de previsão, uma vez que a maioria das violações de margem estão, na verdade, no lado correto do limite de decisão.\n",
    "\n",
    "<img src=\"figuras/svm17.png\" width=\"700\"/>\n",
    "\n",
    "**Parâmetro Gamma**  \n",
    "O parâmetro gamma ajuda o kernel a verificar o nível de influências dos vetores de suporte. É um coeficiente de kernel para 'rbf', 'poli' e 'sigmóide'. Se gamma for definido como 'auto' então 1/n_features serão usados. Valores baixos significam 'alta variância' e maior influência do vetor de suporte e valores altos significam 'baixa variância' e os vetores de suporte não possuem grande influência no processo de classificação. Os parâmetros gama podem ser vistos como o inverso do raio de influência de amostras selecionadas pelo modelo como vetores de suporte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vantagens e Desvantagens do SVM\n",
    "\n",
    "**Vantagens**  \n",
    "- Eficaz em espaços dimensionais elevados.  \n",
    "- Usa um subconjunto de pontos de treinamento na função de decisão (chamados vetores de suporte), o que o torna eficiente na utilização de memória.  \n",
    "- Versátil: diferentes funções de Kernel podem ser especificadas para a função de decisão. Os kernels mais comuns são fornecidos, mas também é possível especificar kernels personalizados.\n",
    "\n",
    "**Desvantagens**\n",
    "- Se o número de características é muito maior do que o número de amostras, o método é susceptível a ter problemas de desempenho.  \n",
    "- As SVM’s não fornecem diretamente estimativas de probabilidade, estas são calculadas usando uma validação cruzada (cross-validation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
