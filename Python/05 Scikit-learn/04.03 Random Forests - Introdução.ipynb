{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests - Um conjunto de decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para classificação:  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "\n",
    "    \n",
    "Para regressão:  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma crítica comum às árvores de decisão é que, uma vez que o conjunto de treinamento é dividido após responder a uma pergunta, não é possível reconsiderar essa decisão. Por exemplo, se dividirmos homens e mulheres, cada questão subsequente será apenas sobre homens ou mulheres, e o método não poderá considerar outro tipo de pergunta (digamos, menos de um ano, independentemente do sexo). As Random Forests tentam introduzir algum nível de randomização em cada etapa, propondo árvores alternativas e combinando-as para obter a previsão final. Esses tipos de algoritmos que consideram vários classificadores que respondem à mesma pergunta são chamados de **ensemble methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembles**  \n",
    "São métodos que combinam multiplos modelos de machine learning para criar modelos ainda mais poderosos. O resultado um tanto surpreendente com tais métodos é que a soma pode ser maior que as partes; isto é, uma maioria de votos entre vários estimadores pode acabar sendo melhor que qualquer um dos estimadores individuais que fazem a votação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forests**  \n",
    "São essencialmente uma coleção de decision trees, onde cada árvore é levemente diferente das outras. Sabemos que as decision trees tendem a overfit, porém se construirmos várias árvores, com cada uma gerando um resultado diferente, podemos reduzir esse overfitting ponderando (através do voto) o resultado de cada árvore.  \n",
    "Random forests (florestas aleatórias) tem seu nome derivado da aleatoriedade injetada na construção das decision trees. Existem duas formas de contruir decisiona trees de maneira aleatória: selecionando os pontos de dados usados para construir uma árvore e selecionando os recursos em cada divisão de teste.\n",
    "\n",
    "<img src=\"figuras/random-forests.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construção de Random Forests**  \n",
    "Para construir um modelo de Random Forest, precisamos decidir sobre o número de árvores a serem construídas (parâmetro ``n_estimators``).  \n",
    "\n",
    "Para construir uma árvore utilizamos o método chamado **bootstrap aggregating**, ou na forma reduzida **bagging**. O bootstrap cria um dataset onde repetidamente retira amostras aleatórias com reposição do dataset original, ou seja, a mesma amostra pode ser escolhia várias vezes. Para ilustrar, vamos dizer que queremos criar um exemplo de bootstrap da lista ['a', 'b', 'c', 'd']. Um possível exemplo de bootstrap seria ['b', 'd', 'd', 'c']. Outra amostra possível seria ['d', 'a', 'd', 'a'].\n",
    "\n",
    "Na construção da random forest, o algoritmo de decision tree é levemente diferente. Em vez de procurar o melhor teste para cada nó, o algoritmo seleciona, em cada nó, aleatoriamente, um subconjunto de features e procura o melhor teste possível envolvendo uma dessas features. A quantidade de features selecionadas é controlada pelo parâmetro ``max_features``.\n",
    "\n",
    "Essa seleção de um subconjunto de features é repetida separadamente em cada nó, de modo que cada nó de uma árvore possa tomar uma decisão usando um subconjunto diferente de features.\n",
    "\n",
    "O bootstrap garante que cada decision tree trabalhe com um conjunto de dados levemente diferente, enquanto que a escolha de um subconjunto de features faz com que cada nó opera em sum subconjunto de recusros diferente. Juntos, esses dois mecanismos garantem que todas as árvores das Random Forests sejam diferentes.\n",
    "\n",
    "Um parâmetro crítico nesse processo é o ``max_features``. Se configuramos para o máximo de fetures disponívies o algoritmo olhará para todas e não haverá aleatoriedade (deixando todas as árvores parecidas); Se configurarmos para apenas uma feature, as divisões não poderão ser escolhidas, sendo feitas somente pela aleatoriedade (e provavelmente as árvores terão que ser profundas para se ajustar aos dados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pontos importantes**  \n",
    "Embora a criação de random forests em um grande conjunto de dados possa ser um pouco demorada, é possível paralelizar facilmente vários núcleos de CPU em um computador. Se você estiver usando um processador multi-core (como quase todos os computadores modernos fazem), você pode usar o parâmetro ``n_jobs`` para ajustar o número de núcleos a serem usados. Usar mais núcleos de CPU resultará em acelerações lineares (usando dois núcleos, o treinamento da floresta aleatória será duas vezes mais rápido), mas especificar ``n_jobs`` maiores que o número de núcleos não ajudará.\n",
    "Você pode definir ``n_jobs = -1`` para usar todos os núcleos em seu computador.\n",
    "\n",
    "Random forests tendem a ter mau desempenho em dados espaciais de altíssima dimensão, como dados de texto. Para este tipo de dados, os modelos lineares podem ser mais apropriados.\n",
    "\n",
    "Random forests requerem mais memória e são mais lentas para treinar e prever do que modelos lineares. Se o tempo e a memória forem importantes em um aplicativo, talvez faça sentido usar um modelo linear.\n",
    "\n",
    "Como descrito acima, ``max_features`` determina o quão aleatório cada árvore é, e um menor max_features reduz o overfitting. Uma boa regra geral de configuração desse parâmetro são max_features = sqrt(n_features) para classificação e max_features = log2(n_factures) para regressão. Adicionar max_features ou max_leaf_nodes pode, às vezes, melhorar o desempenho. Também pode reduzir drasticamente os requisitos de espaço e tempo para treinamento e previsão.\n",
    "\n",
    "Uma boa característica do Random Forest é determinar a importância de cada atributo para a variável de saída estimada (``.feature_importances_``), podendo ser usada para **feature selection**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
